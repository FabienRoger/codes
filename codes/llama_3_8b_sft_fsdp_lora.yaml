# script parameters
model_id: "meta-llama/Meta-Llama-3-8B-Instruct" # Hugging Face model id
max_seq_length: 1024                  # max sequence length for model and padding of the dataset
# max_seq_length: 16                  # max sequence length for model and padding of the dataset
# training parameters
report_to: "none"                      # report metrics to nothing
learning_rate: 0.0002                  # learning rate 2e-4
lr_scheduler_type: "constant"          # learning rate scheduler
num_train_epochs: 1                    # number of training epochs
gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 10                      # log every 10 steps
save_strategy: "no"                      # save checkpoint every epoch
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.1                      # warmup ratio
bf16: true                             # use bfloat16 precision
tf32: true                             # use tf32 precision
seed: 8238974
gradient_checkpointing: false           # use gradient checkpointing to save memory
per_device_train_batch_size: 4
data_parallel: true
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
# per_device_train_batch_size: 12
# fsdp: "full_shard auto_wrap offload" # remove offload if enough GPU memory
# fsdp: "full_shard" # remove offload if enough GPU memory
# fsdp_config:
#   backward_prefetch: "backward_pre"
#   forward_prefetch: "false"
#   use_orig_params: "false"
